{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this notebook is to generate insights from saved notes (represented by URLs here) and cluster them into spaces of similar relevance using LLMs and Semantic Clustering."
      ],
      "metadata": {
        "id": "HanD1heLH3wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install llama-index-core\n",
        "!pip install llama-index-embeddings-huggingface"
      ],
      "metadata": {
        "id": "a_sXdcbwl36P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oukg_7ICLqJu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai\n",
        "!pip install llama-index-llms-together\n",
        "!pip install llama-index-llms-openai\n",
        "!pip install llama-index-llms-groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unstructured\n",
        "!pip install tldextract\n",
        "!pip install kneed"
      ],
      "metadata": {
        "id": "v4lkUronjTwO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the LLM to be used for querying."
      ],
      "metadata": {
        "id": "2KQYkjx8Ja2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using TogetherLLM\n",
        "from llama_index.llms.together import TogetherLLM\n",
        "llm = TogetherLLM(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", api_key=\"your_api_key\")\n",
        "\n",
        "# # Using Groq\n",
        "# from llama_index.llms.groq import Groq\n",
        "# llm =Groq(model=\"mixtral-8x7b-32768\", api_key=\"your_api_key\")\n",
        "\n",
        "# # Using OpenAI\n",
        "# from llama_index.llms.openai import OpenAI as llama_openai\n",
        "# import openai\n",
        "# openai.api_key = \"your_api_key\"\n",
        "# os.environ['openai_key'] = openai.api_key\n",
        "# llm = llama_openai(model=\"gpt-3.5-turbo\", api_key=os.environ.get(\"openai_key\"))"
      ],
      "metadata": {
        "id": "YA3n4Ne7iid3"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the bookmarks we saved from the user's Reddit account in Extracting_bookmarks_from_Reddit.py and we store them each in a llama_index document."
      ],
      "metadata": {
        "id": "Ku9avj2oJxzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from llama_index.core import Document\n",
        "\n",
        "saved_reddit_urls = pd.read_csv('reddit_saved.csv')\n",
        "documents = []\n",
        "for index, row in saved_reddit_urls.iterrows():\n",
        "    doc = Document(text=row['Content'])\n",
        "    doc.metadata['source'] = row['URL']\n",
        "    documents.append(doc)"
      ],
      "metadata": {
        "id": "U_npmltv-swG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load more notes using UnstructuredURLLoader (to give the clusterer more notes to work with)."
      ],
      "metadata": {
        "id": "J4ej4gjFKF35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import download_loader\n",
        "\n",
        "UnstructuredURLLoader = download_loader(\"UnstructuredURLLoader\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEEfGNwcFLrB",
        "outputId": "66f526c7-1f07-4a81-e6d0-556a2a84054d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-b3c179bd7598>:3: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
            "  UnstructuredURLLoader = download_loader(\"UnstructuredURLLoader\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "more_note_urls = [\"https://www.techtarget.com/whatis/definition/large-language-model-LLM\",\n",
        "        \"https://www.techtarget.com/searchenterpriseai/tip/Top-generative-AI-tool-categories\",\n",
        "        \"https://www.techtarget.com/whatis/feature/Top-AI-jobs\",\n",
        "        \"https://www.washingtonpost.com/opinions/2024/02/23/myanman-junta-weakening-collapse/\",\n",
        "        \"https://edition.cnn.com/2024/01/02/economy/interest-rate-cuts-inflation-fed-2024/index.html\",\n",
        "        \"https://explodingtopics.com/blog/economic-trends\",\n",
        "        \"https://www.deeplearning.ai/the-batch/issue-231/\",\n",
        "        \"https://www.nytimes.com/2023/12/08/briefing/ai-dominance.html?auth=login-google1tap&login=google1tap\",\n",
        "        \"https://www.scientificamerican.com/article/what-apples-new-vision-pro-headset-might-do-to-our-brain/#:~:text=Apple's%20ads%20have%20shown%20people,hours%A%20end%E2%80%94and%20even\",\n",
        "        \"https://www.redpoints.com/blog/ai-copyright-infringement/\"]\n",
        "\n",
        "loader = UnstructuredURLLoader(urls=more_note_urls, continue_on_failure=False, headers={\"User-Agent\": \"value\"})\n",
        "more_documents = loader.load_data()"
      ],
      "metadata": {
        "id": "Ncge6o-E_Jzx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents += more_documents"
      ],
      "metadata": {
        "id": "syZqt3ZOD6L-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the LLM to extract meaningful insights from the texts and other relevant metadata info."
      ],
      "metadata": {
        "id": "Yl-1Q-vSMIO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tldextract\n",
        "from datetime import datetime\n",
        "\n",
        "def text_to_insights(documents, llm=llm):\n",
        "\n",
        "  categories = [\"Artificial Intelligence\", \"Web Development\", \"Robotics\", \"Science\", \"Medicine\", \"Business\", \"Politics\", \"Entertainment\", \"Sports\", \"Mathematics\"]\n",
        "\n",
        "  for document in documents:\n",
        "\n",
        "    title = llm.complete(f\"Provide the title of this document.\\n###\\n{document.text}\\n\\\n",
        "                          ###\\nThe answer must be in this format: Title: some_title.\\nGive a DIRECT answer.\").text.strip()\n",
        "    category = llm.complete(f\"From the categories in this list: {categories}, provide the one that is best related to this document. The category must be from the ones listed. \\n###\\n{document.text}\\n###\\n\\\n",
        "                          \\nThe answer must be in this format: Category: some_category.\\nGive a DIRECT answer.\").text.strip()\n",
        "    topic = llm.complete(f\"Provide the main idea of this document in one concise sentence. Give a DIRECT and SHORT answer. \\n###\\n{document.text}\\n###\\n\\\n",
        "                          \\nThe answer must be in this format: Main idea: some_main_idea.\").text.strip()\n",
        "\n",
        "    document.metadata['title'] = title.split(\": \")[1]\n",
        "    document.metadata['category'] = category.split(\": \")[1]\n",
        "    document.metadata['topic'] = topic.split(\": \")[1]\n",
        "    if document.metadata['source']:\n",
        "      document.metadata['URL'] = document.metadata['source']\n",
        "      source = tldextract.extract(document.metadata['URL']).domain\n",
        "      document.metadata['source'] = source\n",
        "    document.metadata['date'] = datetime.today().strftime('%Y-%m-%d')\n",
        "    query = f\"Give me the summary of this document as the most relevant bullet points: \\n \\\n",
        "          ###\\n \\\n",
        "          {document.text}\\n \\\n",
        "          ### \\n \\\n",
        "          Be concise, avoid redundant ideas and provide the minimum number of bullet points without missing an important point.\"\n",
        "    resp = llm.complete(query)\n",
        "    document.text = resp.text.strip()\n",
        "  return documents"
      ],
      "metadata": {
        "id": "RPziOMyCFj-t"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = text_to_insights(documents, llm)\n",
        "print(documents[1].text)\n",
        "print(documents[1].metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7k1ddqmFn78",
        "outputId": "a7000227-dcce-4b8e-c625-bdb6137d973e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- 2024 marks the year for GroqChip's potential dominance among AI startups due to its affordability and speed.\n",
            "- GroqChip's US manufacturing differentiates it from competitors, avoiding regulatory uncertainties tied to chips from China.\n",
            "- GroqChip outperforms industry competitors in throughput (speed) versus price, being 18 times faster for LLM inference performance.\n",
            "- With on-chip memory, GroqChip is quicker and has lower manufacturing costs than competitors using off-chip memory.\n",
            "- Designed for open-source LLMs like Nistral, allowing scalability as models grow more powerful.\n",
            "- Target market: small and middle-sized LLM startups, focusing on chatbots and customer service applications for its super fast latency.\n",
            "- Groq aims to produce 1 million chips by the end of 2024 to address demand.\n",
            "- Main concern: GroqChip may require revolutionary design changes to handle 10 trillion parameter models in the future.\n",
            "- Additional points:\n",
            "  - Potential for image-based models.\n",
            "  - Investment opportunities for Groq.\n",
            "  - Concerns about the functionality of Groq's chat on their website.\n",
            "{'source': 'reddit', 'title': \"GroqChip's Dominance in 2024 AI Startup Market\", 'category': 'Artificial Intelligence', 'topic': 'GroqChip, a US-manufactured, affordable, and fast AI chip designed for open-source large language models, is expected to be the chip of choice for small and middle-sized AI startups in 2024, particularly for chatbot and customer service applications, with a focus on scaling and speed efficiency.', 'URL': 'reddit', 'date': '2024-03-05'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A filter that we could use to filter documents by category and date."
      ],
      "metadata": {
        "id": "HaSFmoECMspM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_documents(documents, category=False, date=False):\n",
        "  if category and date:\n",
        "    date_as_datetime = datetime.strptime(date, '%Y-%m-%d')\n",
        "    return [doc for doc in documents if (datetime.strptime(doc.metadata['date'], '%Y-%m-%d') >= date_as_datetime) and (doc.metadata['category'] == category)]\n",
        "  elif category:\n",
        "    return [doc for doc in documents if (doc.metadata['category'] == category)]\n",
        "  elif date:\n",
        "    date_as_datetime = datetime.strptime(date, '%Y-%m-%d')\n",
        "    return [doc for doc in documents if (doc.metadata['date'] >= date_as_datetime)]\n",
        "  else:\n",
        "    return documents"
      ],
      "metadata": {
        "id": "1BRp1fDKcOFM"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function to extract global insights from a set of documents."
      ],
      "metadata": {
        "id": "xSOM9szSNpBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_global_insights(documents, llm=llm):\n",
        "  all_insights = \"\\n\".join([doc.text for doc in documents])\n",
        "  query = f\"Synthetize this document and provide the most important ideas as bullet points: \\n \\\n",
        "          ###\\n \\\n",
        "          {all_insights}\\n \\\n",
        "          ### \\n \\\n",
        "          Be concise, avoid redundant ideas and provide the minimum number of bullet points without missing an important point.\"\n",
        "  resp = llm.complete(query)\n",
        "  return resp.text.strip()"
      ],
      "metadata": {
        "id": "1kLAW3CPuLJc"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(extract_global_insights(documents[:4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m33iLSLA8P_",
        "outputId": "1fc1905e-a5f5-4bfa-b0a9-b284746add6b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- AI can be beneficial for school work by confirming work, helping with brainstorming and customizing lessons, but it's important to verify the accuracy of AI-generated information and avoid plagiarism.\n",
            "- Experienced users recommend using AI for generating papers, summarizing after reading the material, and editing work.\n",
            "- AI should be used as an aid to augment abilities, not as a replacement for critical thinking and problem-solving skills.\n",
            "- GroqChip, an AI startup, is expected to dominate the market in 2024 due to its affordability, speed, and US manufacturing.\n",
            "- GroqChip outperforms industry competitors in throughput versus price and is designed for open-source LLMs, allowing scalability as models grow more powerful.\n",
            "- Groq's target market is small and middle-sized LLM startups, focusing on chatbots and customer service applications.\n",
            "- Groq aims to produce 1 million chips by the end of 2024 to address demand, but may require revolutionary design changes to handle 10 trillion parameter models in the future.\n",
            "- Sberbank's Robotics Center is hiring AI specialists to develop AI for humanoid robots for use in controlling manipulators, mobile wheeled robots, and robotic dogs.\n",
            "- Sberbank aims to become a human-centered organization using AI to help individuals realize their potential and manage technology and information excess.\n",
            "- Other AI news includes the introduction of advanced refinement strategies with stepwise outcome-based and process-based reward models, a new AI worm infecting users through AI-enabled email clients, a Korean AI image generator outperforming OpenAI's tool, and AI-generated porn persisting on Etsy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clustering**"
      ],
      "metadata": {
        "id": "143rmNjUOUxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "def embed_doc_text(document, embedding_model=embedding_model):\n",
        "  embedding = embedding_model.get_text_embedding(document.metadata['topic'])\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "PNteB9XG1WTw"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "documents_embeddings = []\n",
        "document_to_embedding = {}\n",
        "embedding_to_document = {}\n",
        "for idx, document in enumerate(documents):\n",
        "  documents_embeddings.append(embed_doc_text(document))\n",
        "  document_to_embedding[document.id_] = idx\n",
        "  embedding_to_document[idx] = document.id_\n",
        "\n",
        "documents_embeddings = np.array(documents_embeddings)"
      ],
      "metadata": {
        "id": "_AHDJzvVQPY-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic KMeans: adapted from https://github.com/avanwyk/semantic-document-clustering/blob/master/clustering.py\n"
      ],
      "metadata": {
        "id": "GRUPRlJ5Oh5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def SemanticKMeans_clustering(embeddings: np.ndarray, centroids: int) -> np.ndarray:\n",
        "    kmeans = SemanticKMeans(centroids)\n",
        "    predictions = kmeans.fit_predict(embeddings).reshape(-1, 1)\n",
        "    inertia = kmeans.inertia_\n",
        "    return predictions, inertia\n",
        "\n",
        "class SemanticKMeans:\n",
        "    def __init__(self, centroids):\n",
        "        self.num_centroids = centroids\n",
        "        self.inertia_ = None\n",
        "\n",
        "    def fit(self, vectors: np.ndarray, max_iterations: int = 200, random_state: int = 2024) -> 'SemanticKMeans':\n",
        "        np.random.seed(random_state)\n",
        "        centroids = np.random.uniform(-1., 1.,\n",
        "                                      (self.num_centroids, vectors.shape[1]))\n",
        "\n",
        "        for _ in range(max_iterations):\n",
        "            distances = cosine_similarity(vectors, centroids).clip(-1., 1.)\n",
        "            prev_centroids = np.copy(centroids)\n",
        "            for c in range(self.num_centroids):\n",
        "                members = vectors[np.argmax(distances, axis=1) == c]\n",
        "                if len(members) > 0:\n",
        "                    centroids[c] = np.mean(members, axis=0)\n",
        "\n",
        "            if np.allclose(centroids, prev_centroids):\n",
        "                break\n",
        "        self.centroids = centroids\n",
        "        self.inertia_ = self.calculate_inertia(vectors, centroids)\n",
        "        return self\n",
        "\n",
        "    def centroids_(self):\n",
        "        return self.centroids\n",
        "\n",
        "    def calculate_inertia(self, vectors: np.ndarray, centroids: np.ndarray) -> float:\n",
        "        distances = cosine_similarity(vectors, centroids)\n",
        "        labels = np.argmax(distances, axis=1)\n",
        "        inertia = 0.0\n",
        "        for c in range(self.num_centroids):\n",
        "            members = vectors[labels == c]\n",
        "            if len(members) > 0:\n",
        "                inertia += np.sum((members - centroids[c])**2)\n",
        "        return inertia\n",
        "\n",
        "    def predict(self, vectors: np.ndarray) -> np.ndarray:\n",
        "        distances = cosine_similarity(vectors, self.centroids)\n",
        "        return np.argmax(distances, axis=1)\n",
        "\n",
        "    def fit_predict(self, vectors: np.ndarray, max_iterations: int = 100) -> np.ndarray:\n",
        "        return self.fit(vectors, max_iterations).predict(vectors)"
      ],
      "metadata": {
        "id": "hHQwcUxiFAaR"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kneed import KneeLocator\n",
        "\n",
        "def cluster_documents(X):\n",
        "  inertia = []\n",
        "  k_range = range(1, min(20, X.shape[0]))\n",
        "  for k in k_range:\n",
        "      labels, inertia_ = SemanticKMeans_clustering(X, k)\n",
        "      inertia.append(inertia_)\n",
        "\n",
        "  # We use KneeLocator from the kneed library to perform the elbow method without human intervention.\n",
        "  knee = KneeLocator(list(k_range), inertia, curve='convex', direction='decreasing')\n",
        "  optimal_k = knee.knee\n",
        "  n_clusters = 2 # Default value if the KneeLocator does not find the elbow.\n",
        "  if optimal_k:\n",
        "    n_clusters = optimal_k\n",
        "  labels, _ = SemanticKMeans_clustering(X, n_clusters)\n",
        "  return labels"
      ],
      "metadata": {
        "id": "-1Nnl4BVG3Rd"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = cluster_documents(documents_embeddings)\n",
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkmMGUc2WjGY",
        "outputId": "caeafd79-b129-4df6-e26f-97620da73e79"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2],\n",
              "       [3],\n",
              "       [2],\n",
              "       [2],\n",
              "       [1],\n",
              "       [3],\n",
              "       [3],\n",
              "       [2],\n",
              "       [0],\n",
              "       [3],\n",
              "       [3],\n",
              "       [3],\n",
              "       [2],\n",
              "       [3],\n",
              "       [3]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We group the documents by label, then we generate a unifying topic to the insight space."
      ],
      "metadata": {
        "id": "Z5plbQRy4q4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = np.unique(labels)\n",
        "insight_spaces = {}\n",
        "for label in unique_labels:\n",
        "  embedding_indexes = np.where((labels == label))[0]\n",
        "  documents_ids = [embedding_to_document[idx] for idx in embedding_indexes]\n",
        "  insight_spaces[label] = {'documents_ids': documents_ids, 'space_topic': \"\"}\n"
      ],
      "metadata": {
        "id": "d6gMXtBFQO9K"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for space_id in insight_spaces.keys():\n",
        "  docs_topics = [doc.metadata['topic'] for doc in documents if (doc.id_ in insight_spaces[space_id]['documents_ids'])]\n",
        "  query = f\"Given this list of topics:\\n###\\n{docs_topics}\\n###\\nProvide a unifying topic in 15 words or less.\"\n",
        "  resp = llm.complete(query).text.strip()\n",
        "  insight_spaces[space_id]['space_topic'] = resp"
      ],
      "metadata": {
        "id": "BvLVGp85WRJ-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insight_spaces"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZXcncz3ZDt5",
        "outputId": "b96d4022-0f30-4656-c406-f6934028ff4a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {'documents_ids': ['45f828d4-9033-4f50-aaa3-862e308c8459'],\n",
              "  'space_topic': '\"Exploring the Crisis in Myanmar: Military Junta, Insurgents, and International Response.\"'},\n",
              " 1: {'documents_ids': ['10bcd969-53c3-4664-838a-c4a5fa2bc7d5'],\n",
              "  'space_topic': '\"Elon Musk\\'s Involvement and Concerns over AI Development and Safety.\"'},\n",
              " 2: {'documents_ids': ['8d9f757a-daf3-4e02-a5a1-5169ce96a67f',\n",
              "   'fb2bbd36-3df6-4dc7-bdaa-4750ab3e1919',\n",
              "   '96692932-86ae-4765-9d2a-18761f814d90',\n",
              "   '67454731-5f48-4e4d-b333-7f7e8fbd42b2',\n",
              "   'ab1fc6be-f143-4246-a3d5-f36197bc3ce3'],\n",
              "  'space_topic': 'Exploring the impact and ethical considerations of AI in various industries.'},\n",
              " 3: {'documents_ids': ['a3da183b-8d9b-4163-a974-f50b09b0a009',\n",
              "   '1e1b9f9f-7c21-412f-a643-654977134421',\n",
              "   'bb630c8f-73a1-4c90-8573-61071f926cf1',\n",
              "   '0cb5edba-8d3d-4004-8ce5-5f3fa284db75',\n",
              "   '9c9bc931-7df7-4b7e-9754-f18f9b7d2417',\n",
              "   '06b5613f-426d-4362-81a8-708cde6c0b13',\n",
              "   '92057d9f-ccfa-4e83-ac60-714099cc87ca',\n",
              "   '2d56adfc-a865-4b24-aa53-3560486fcc9c'],\n",
              "  'space_topic': 'Advances in Technology: AI Chips, Generative AI, VR Headsets, and Brand Protection.'}}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NcGdGaI3d5fy"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}